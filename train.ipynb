{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import division\n",
    "#from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from utils import *\n",
    "from modules import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Namespace(batch_size=128, cuda=False, decoder='rnn', decoder_dropout=0.0, decoder_hidden=256, dims=4, dynamic_graph=False, edge_types=2, encoder='mlp', encoder_dropout=0.0, encoder_hidden=256, epochs=50, factor=True, gamma=0.5, hard=False, load_folder='', lr=0.0005, lr_decay=200, no_cuda=True, no_factor=False, num_atoms=5, prediction_steps=10, prior=False, save_folder='logs', seed=42, skip_first=False, suffix='_springs5', temp=0.5, timesteps=49, var=5e-05)\n"
     ]
    }
   ],
   "source": [
    "##### set params\n",
    "\n",
    "parser = argparse.ArgumentParser('-cuda')\n",
    "\n",
    "\n",
    "parser.add_argument('--no-cuda', action='store_true', default=True,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=500,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--batch-size', type=int, default=128,\n",
    "                    help='Number of samples per batch.')\n",
    "parser.add_argument('--lr', type=float, default=0.0005,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--encoder-hidden', type=int, default=256,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--decoder-hidden', type=int, default=256,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--temp', type=float, default=0.5,\n",
    "                    help='Temperature for Gumbel softmax.')\n",
    "parser.add_argument('--num-atoms', type=int, default=5,\n",
    "                    help='Number of atoms in simulation.')\n",
    "parser.add_argument('--encoder', type=str, default='mlp',\n",
    "                    help='Type of path encoder model (mlp or cnn).')\n",
    "parser.add_argument('--decoder', type=str, default='rnn',\n",
    "                    help='Type of decoder model (mlp, rnn, or sim).')\n",
    "parser.add_argument('--no-factor', action='store_true', default=False,\n",
    "                    help='Disables factor graph model.')\n",
    "parser.add_argument('--suffix', type=str, default='_springs5',\n",
    "                    help='Suffix for training data (e.g. \"_charged\".')\n",
    "parser.add_argument('--encoder-dropout', type=float, default=0.0,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--decoder-dropout', type=float, default=0.0,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--save-folder', type=str, default='logs',\n",
    "                    help='Where to save the trained model, leave empty to not save anything.')\n",
    "parser.add_argument('--load-folder', type=str, default='',\n",
    "                    help='Where to load the trained model if finetunning. ' +\n",
    "                         'Leave empty to train from scratch')\n",
    "parser.add_argument('--edge-types', type=int, default=2,\n",
    "                    help='The number of edge types to infer.')\n",
    "parser.add_argument('--dims', type=int, default=4,\n",
    "                    help='The number of input dimensions (position + velocity).')\n",
    "parser.add_argument('--timesteps', type=int, default=49,\n",
    "                    help='The number of time steps per sample.')\n",
    "parser.add_argument('--prediction-steps', type=int, default=10, metavar='N',\n",
    "                    help='Num steps to predict before re-using teacher forcing.')\n",
    "parser.add_argument('--lr-decay', type=int, default=200,\n",
    "                    help='After how epochs to decay LR by a factor of gamma.')\n",
    "parser.add_argument('--gamma', type=float, default=0.5,\n",
    "                    help='LR decay factor.')\n",
    "parser.add_argument('--skip-first', action='store_true', default=False,\n",
    "                    help='Skip first edge type in decoder, i.e. it represents no-edge.')\n",
    "parser.add_argument('--var', type=float, default=5e-5,\n",
    "                    help='Output variance.')\n",
    "parser.add_argument('--hard', action='store_true', default=False,\n",
    "                    help='Uses discrete samples in training forward pass.')\n",
    "parser.add_argument('--prior', action='store_true', default=False,\n",
    "                    help='Whether to use sparsity prior.')\n",
    "parser.add_argument('--dynamic-graph', action='store_true', default=False,\n",
    "                    help='Whether test with dynamically re-computed graph.')\n",
    "\n",
    "args, unknown = parser.parse_known_args('--epochs 50'.split())\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "args.factor = not args.no_factor\n",
    "print(args)\n",
    "\n",
    "\n",
    "################## set params end ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# save log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "if args.dynamic_graph:\n",
    "    print(\"Testing with dynamically re-computed graph.\")\n",
    "\n",
    "# Save model and meta-data. Always saves in a new sub-folder.\n",
    "if args.save_folder:\n",
    "    exp_counter = 0\n",
    "    now = datetime.datetime.now()\n",
    "    timestamp = now.isoformat()\n",
    "    save_folder = '{}/exp{}/'.format(args.save_folder, timestamp)\n",
    "    try:\n",
    "        os.makedirs(save_folder)\n",
    "    except:\n",
    "        pass\n",
    "    meta_file = os.path.join(save_folder, 'metadata.pkl')\n",
    "    encoder_file = os.path.join(save_folder, 'encoder.pt')\n",
    "    decoder_file = os.path.join(save_folder, 'decoder.pt')\n",
    "\n",
    "    log_file = os.path.join(save_folder, 'log.txt')\n",
    "    log = open(log_file, 'w')\n",
    "\n",
    "    pickle.dump({'args': args}, open(meta_file, \"wb\"))\n",
    "else:\n",
    "    print(\"WARNING: No save_folder provided!\" +\n",
    "          \"Testing (within this script) will throw an error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# prepare data\n",
    "\n",
    "\n",
    "+ Load data from a pytorch dataloader instance (create in util)\n",
    "    + contains simulation data\n",
    "\n",
    "+ processing graph data \n",
    "    + a 5*5 adjacent matrix -> sender and receiver\n",
    "        + sender: one-hot form of row index of non-zero off-diagoal element  \n",
    "        + receiver one-hot form of column index of non-zero off-diagoal element\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch dataloader \n",
    "train_loader, valid_loader, test_loader, loc_max, loc_min, vel_max, vel_min = load_data(\n",
    "    args.batch_size, args.suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate off-diagonal interaction graph\n",
    "\n",
    "off_diag = np.ones([args.num_atoms, args.num_atoms]) - np.eye(args.num_atoms)\n",
    "\n",
    "rel_rec = np.array(encode_onehot(np.where(off_diag)[0]), dtype=np.float32)\n",
    "rel_send = np.array(encode_onehot(np.where(off_diag)[1]), dtype=np.float32)\n",
    "rel_rec = torch.FloatTensor(rel_rec)\n",
    "rel_send = torch.FloatTensor(rel_send)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate off-diagonal interaction graph\n",
    "\n",
    "off_diag = np.ones([args.num_atoms, args.num_atoms]) - np.eye(args.num_atoms)\n",
    "\n",
    "rel_rec = np.array(encode_onehot(np.where(off_diag)[0]), dtype=np.float32)\n",
    "rel_send = np.array(encode_onehot(np.where(off_diag)[1]), dtype=np.float32)\n",
    "rel_rec = torch.FloatTensor(rel_rec)\n",
    "rel_send = torch.FloatTensor(rel_send)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "idx(row idx and column idx) of non-zero element from off-diags \n (array([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4]), array([1, 2, 3, 4, 0, 2, 3, 4, 0, 1, 3, 4, 0, 1, 2, 4, 0, 1, 2, 3]))\n"
     ]
    }
   ],
   "source": [
    "print('idx(row idx and column idx) of non-zero element from off-diags \\n',np.where(off_diag)) #  idx of non-zero element of off-diag matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "off_diag = \n [[0. 1. 1. 1. 1.]\n [1. 0. 1. 1. 1.]\n [1. 1. 0. 1. 1.]\n [1. 1. 1. 0. 1.]\n [1. 1. 1. 1. 0.]]\ntorch.Size([20, 5])\ntensor([[1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [0., 1., 0., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 1., 0., 0.],\n        [0., 0., 0., 1., 0.],\n        [0., 0., 0., 1., 0.],\n        [0., 0., 0., 1., 0.],\n        [0., 0., 0., 1., 0.],\n        [0., 0., 0., 0., 1.],\n        [0., 0., 0., 0., 1.],\n        [0., 0., 0., 0., 1.],\n        [0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "print('off_diag = \\n', off_diag)\n",
    "print(rel_rec.shape)\n",
    "print(rel_rec) # change row idx (recevier) to one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using factor graph MLP encoder.\n"
     ]
    }
   ],
   "source": [
    "encoder = MLPEncoder(args.timesteps * args.dims, args.encoder_hidden,\n",
    "                         args.edge_types,\n",
    "                         args.encoder_dropout, args.factor)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Decoder\n",
    "\n",
    "task: predict $$p_{\\theta}\\left(\\mathbf{x}^{t+1} \\mid \\mathbf{x}^{t}, \\cdots, \\mathbf{x}^{1}, \\mathbf{z}\\right)$$ \n",
    "\n",
    "(under markov $$p_{\\theta}\\left(\\mathbf{x}^{t+1} \\mid \\mathbf{x}^{t}, \\cdots, \\mathbf{x}^{1}, \\mathbf{z}\\right)=p_{\\theta}\\left(\\mathbf{x}^{t+1} \\mid \\mathbf{x}^{t}, \\mathbf{z}\\right)$$)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "`rnn`\n",
    "\n",
    "\\begin{aligned}\n",
    "v \\rightarrow e: \\tilde{\\mathbf{h}}_{(i, j)}^{t} &=\\sum_{k} z_{i j, k} \\tilde{f}_{e}^{k}\\left(\\left[\\tilde{\\mathbf{h}}_{i}^{t}, \\tilde{\\mathbf{h}}_{j}^{t}\\right]\\right) \\\\\n",
    "e \\rightarrow v: \\operatorname{MSG}_{j}^{t} &=\\sum_{i \\neq j} \\tilde{\\mathbf{h}}_{(i, j)}^{t} \\\\\n",
    "\\tilde{\\mathbf{h}}_{j}^{t+1} &=\\operatorname{GRU}\\left(\\left[\\operatorname{MSG}_{j}^{t}, \\mathbf{x}_{j}^{t}\\right], \\tilde{\\mathbf{h}}_{j}^{t}\\right) \\\\\n",
    "\\boldsymbol{\\mu}_{j}^{t+1} &=\\mathbf{x}_{j}^{t}+f_{\\text {out }}\\left(\\tilde{\\mathbf{h}}_{j}^{t+1}\\right) \\\\\n",
    "p\\left(\\mathbf{x}^{t+1} \\mid \\mathbf{x}^{t}, \\mathbf{z}\\right) &=\\mathcal{N}\\left(\\boldsymbol{\\mu}^{t+1}, \\sigma^{2} \\mathbf{I}\\right)\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDecoder(nn.Module):\n",
    "    def __init__(self, n_in_node, edge_types, n_hid, do_prob = 0., skip_first=False):\n",
    "        '''\n",
    "        About params\n",
    "            + n_in_node: number of balls = 5 in default\n",
    "            + edge_types: arbitrary num, = 2 in default (connected or unconnected)\n",
    "            + n_hid: a hyper params, set the num of units of inner layer of decoder, = 256 in default\n",
    "\n",
    "\n",
    "        '''\n",
    "        super(RNNDecoder, self).__init__()\n",
    "        \n",
    "        # pre define some nn blocks\n",
    "        self.msg_fc1 = nn.ModuleList(\n",
    "            [nn.Linear(2 * n_hid, n_hid) for _ in range(edge_types)])\n",
    "        self.msg_fc2 = nn.ModuleList(\n",
    "            [nn.Linear(n_hid, n_hid) for _ in range(edge_types)])\n",
    "        self.msg_out_shape = n_hid\n",
    "        self.skip_first_edge_type = skip_first\n",
    "\n",
    "        self.hidden_r = nn.Linear(n_hid, n_hid, bias=False)\n",
    "        self.hidden_i = nn.Linear(n_hid, n_hid, bias=False)\n",
    "        self.hidden_h = nn.Linear(n_hid, n_hid, bias=False)\n",
    "\n",
    "        self.input_r = nn.Linear(n_in_node, n_hid, bias=True)\n",
    "        self.input_i = nn.Linear(n_in_node, n_hid, bias=True)\n",
    "        self.input_n = nn.Linear(n_in_node, n_hid, bias=True)\n",
    "\n",
    "        self.out_fc1 = nn.Linear(n_hid, n_hid)\n",
    "        self.out_fc2 = nn.Linear(n_hid, n_hid)\n",
    "        self.out_fc3 = nn.Linear(n_hid, n_in_node)\n",
    "    \n",
    "        print('Using learned recurrent interaction net decoder.')\n",
    "\n",
    "        self.dropout_prob = do_prob\n",
    "        \n",
    "    def single_step_forward(self, inputs, rel_rec, rel_send, rel_type, hidden):\n",
    "        '''\n",
    "        Single: means a single timestep \n",
    "        \n",
    "        arguments:\n",
    "            + inputs:(a timestep)             + inputs: \n",
    "                                            + 'data', vel and loc [batch_size, num_atoms, num_timesteps, num_dims * num_variables]\n",
    "                                                    + in example: train data shape torch.Size([128, 49, 5, 4])\n",
    "                                                        + batch_size = 128\n",
    "                                                        + ball num\n",
    "                                                        + timesteps records\n",
    "                                                        + 2-dim vel + 2-dim loc\n",
    "                    + ins = inputs[:, step, :, :]\n",
    "                        + [128, 5, 4]\n",
    "                        \n",
    "            + rel_rec:\n",
    "                + [num of non-zero-off-diagonal element(in this case, 5*5 - 5),  n_balls(encoding dim)]\n",
    "                    + in example :rel_rec shape torch.Size([20, 5])\n",
    "            \n",
    "            + rel_send:\n",
    "                + same as rel_rec\n",
    "            \n",
    "            + rel_type: (for extracting info from a specific type edge)\n",
    "                + [batchsize, num of non-zero-off-diagonal element, n_type] \n",
    "                    + in eg: rel_type shape torch.Size([128, 20, 2])\n",
    "            \n",
    "            + hidden: 256(hyperparams)\n",
    "                \n",
    "                    \n",
    "\n",
    "        '''\n",
    "\n",
    "        \n",
    "        ############## node2edge\n",
    "        \n",
    "        ######## (13)\n",
    "        #print('inputs', inputs.shape)\n",
    "        receivers = torch.matmul(rel_rec, hidden) #  fe pass hidden (20, 5) (128, 5, 256) -> 128 20 256              \n",
    "        senders = torch.matmul(rel_send, hidden) #  fe\n",
    "        pre_msg = torch.cat([senders, receivers], dim=-1)   # contatenate # (13 [h, h])\n",
    "        #print('pre_msg size', pre_msg.shape) # pre_msg size torch.Size([128, 20, 512])\n",
    "        \n",
    "        # init zeros... for later fill out\n",
    "        all_msgs = Variable(torch.zeros(pre_msg.size(0), pre_msg.size(1),\n",
    "                                        self.msg_out_shape)) # (128, 20, 256) # msg_out_shape=256=n_hid\n",
    "                     \n",
    "        ############# ignore\n",
    "        if inputs.is_cuda:\n",
    "            all_msgs = all_msgs.cuda()\n",
    "\n",
    "            \n",
    "            \n",
    "        if self.skip_first_edge_type:\n",
    "            start_idx = 1\n",
    "            norm = float(len(self.msg_fc2)) - 1.\n",
    "        else:\n",
    "            start_idx = 0\n",
    "            norm = float(len(self.msg_fc2))\n",
    "\n",
    "            \n",
    "        \n",
    "        ############### edge2node \n",
    "        ####### (14)\n",
    "        # Run separate MLP for every edge type (in this case, 2)\n",
    "        # Only take graph edge-type, structure info\n",
    "        for i in range(start_idx, len(self.msg_fc2)):            \n",
    "            msg = F.tanh(self.msg_fc1[i](pre_msg)) # 512 -> 256\n",
    "            msg = F.dropout(msg, p=self.dropout_prob)\n",
    "            msg = F.tanh(self.msg_fc2[i](msg)) # 256 ->256\n",
    "            msg = msg * rel_type[:, :, i:i + 1]   # only extract edge type i \n",
    "            all_msgs += msg / norm   \n",
    "        # (128, 256, 20) (20, 5) = (128, 256, 5)....5 balls receive msg from others (4 each, 20 total)\n",
    "        agg_msgs = all_msgs.transpose(-2, -1).matmul(rel_rec).transpose(-2, -1)# all msgs \n",
    "        agg_msgs = agg_msgs.contiguous() / inputs.size(2)  # agg_msgs torch.Size([128, 5, 256])\n",
    "        #print('agg_msgs', agg_msgs.shape)\n",
    "        \n",
    "        ####### (15)\n",
    "        # GRU-style gated aggregation\n",
    "        # single input (128, 5 , 4) -> (128, 5, 256), sum... GRU style\n",
    "        r = F.sigmoid(self.input_r(inputs) + self.hidden_r(agg_msgs))  \n",
    "        i = F.sigmoid(self.input_i(inputs) + self.hidden_i(agg_msgs))     \n",
    "        n = F.tanh(self.input_n(inputs) + r * self.hidden_h(agg_msgs))\n",
    "        hidden = (1 - i) * n + i * hidden   \n",
    "        \n",
    "        ####### (16 right) \n",
    "        # Output MLP f_out\n",
    "        pred = F.dropout(F.relu(self.out_fc1(hidden)), p=self.dropout_prob)\n",
    "        pred = F.dropout(F.relu(self.out_fc2(pred)), p=self.dropout_prob)\n",
    "        pred = self.out_fc3(pred)\n",
    "\n",
    "        ####### (16)\n",
    "        # Predict position/velocity difference\n",
    "        pred = inputs + pred \n",
    "\n",
    "        return pred, hidden\n",
    "\n",
    "    def forward(self, data, rel_type, rel_rec, rel_send, pred_steps=1,\n",
    "                burn_in=False, burn_in_steps=1, dynamic_graph=False,\n",
    "                encoder=None, temp=None):\n",
    "        '''\n",
    "        data torch.Size([128, 5, 49, 4])\n",
    "        rel_type torch.Size([128, 20, 2])\n",
    "        rel_rec torch.Size([20, 5])\n",
    "        rel_send torch.Size([20, 5])\n",
    "        \n",
    "        '''\n",
    "        #print('data', data.shape)  \n",
    "        #print('rel_type', rel_type.shape)\n",
    "        #print('rel_rec', rel_rec.shape)\n",
    "        #print('rel_send', rel_send.shape)\n",
    "        \n",
    "        \n",
    "        ##### ignore\n",
    "        inputs = data.transpose(1, 2).contiguous() #  ([128, 5, 49, 4]) -> ([128, 49, 5, 4])\n",
    "        time_steps = inputs.size(1)\n",
    "        \n",
    "        hidden = Variable(\n",
    "            torch.zeros(inputs.size(0), inputs.size(2), self.msg_out_shape)) \n",
    "        \n",
    "        if inputs.is_cuda:\n",
    "            hidden = hidden.cuda()\n",
    "\n",
    "        pred_all = []\n",
    "        \n",
    "        \n",
    "        ####### timesteps loop\n",
    "        for step in range(0, inputs.size(1) - 1):\n",
    "            \n",
    "            # if < , use self.info\n",
    "            if burn_in:\n",
    "                if step <= burn_in_steps:\n",
    "                    ins = inputs[:, step, :, :]\n",
    "                else:\n",
    "                    ins = pred_all[step - 1] \n",
    "            else:\n",
    "                assert (pred_steps <= time_steps)\n",
    "                # Use ground truth trajectory input vs. last prediction\n",
    "                if not step % pred_steps:\n",
    "                    ins = inputs[:, step, :, :]\n",
    "                else:\n",
    "                    ins = pred_all[step - 1]\n",
    "\n",
    "            if dynamic_graph and step >= burn_in_steps:\n",
    "                # NOTE: Assumes burn_in_steps = args.timesteps\n",
    "                logits = encoder(\n",
    "                    data[:, :, step - burn_in_steps:step, :].contiguous(),\n",
    "                    rel_rec, rel_send)\n",
    "                # logits shape =  torch.Size([128, 20, 2])\n",
    "                rel_type = gumbel_softmax(logits, tau=temp, hard=True)\n",
    "                # sampled.. rel_type ([128, 20, 2])\n",
    "            pred, hidden = self.single_step_forward(ins, rel_rec, rel_send,\n",
    "                                                    rel_type, hidden)\n",
    "            pred_all.append(pred)\n",
    "\n",
    "        preds = torch.stack(pred_all, dim=1)\n",
    "\n",
    "        return preds.transpose(1, 2).contiguous()        \n",
    "        \n",
    "        #print(\"rel_type shape\", rel_type.shape)\n",
    "        # node2edge\n",
    "        receivers = torch.matmul(rel_rec, hidden) # receivers shape =  torch.Size([128, 20, 256])\n",
    "                                                # (128, 20 ,5 ) * ([128, 5, 256])\n",
    "        senders = torch.matmul(rel_send, hidden)\n",
    "                                                \n",
    "        pre_msg = torch.cat([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using learned recurrent interaction net decoder.\n"
     ]
    }
   ],
   "source": [
    "# use rnn\n",
    "decoder = RNNDecoder(n_in_node=args.dims,\n",
    "                     edge_types=args.edge_types,\n",
    "                     n_hid=args.decoder_hidden,\n",
    "                     do_prob=args.decoder_dropout,\n",
    "                     skip_first=args.skip_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.load_folder:\n",
    "    encoder_file = os.path.join(args.load_folder, 'encoder.pt')\n",
    "    encoder.load_state_dict(torch.load(encoder_file))\n",
    "    decoder_file = os.path.join(args.load_folder, 'decoder.pt')\n",
    "    decoder.load_state_dict(torch.load(decoder_file))\n",
    "\n",
    "    args.save_folder = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()),\n",
    "                       lr=args.lr)\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=args.lr_decay,\n",
    "                                gamma=args.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear indices of an upper triangular mx, used for acc calculation\n",
    "triu_indices = get_triu_offdiag_indices(args.num_atoms)\n",
    "tril_indices = get_tril_offdiag_indices(args.num_atoms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.prior:\n",
    "    ### default 不用这个 (用 1/2， 1/2)\n",
    "    prior = np.array([0.91, 0.03, 0.03, 0.03])  # TODO: hard coded for now\n",
    "    print(\"Using prior\")\n",
    "    print(prior)\n",
    "    log_prior = torch.FloatTensor(np.log(prior))\n",
    "    log_prior = torch.unsqueeze(log_prior, 0)\n",
    "    log_prior = torch.unsqueeze(log_prior, 0)\n",
    "    log_prior = Variable(log_prior)\n",
    "\n",
    "    if args.cuda:\n",
    "        log_prior = log_prior.cuda()\n",
    "\n",
    "if args.cuda:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "    rel_rec = rel_rec.cuda()\n",
    "    rel_send = rel_send.cuda()\n",
    "    triu_indices = triu_indices.cuda()\n",
    "    tril_indices = tril_indices.cuda()\n",
    "\n",
    "rel_rec = Variable(rel_rec)\n",
    "rel_send = Variable(rel_send)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch, best_val_loss):\n",
    "    t = time.time()\n",
    "    nll_train = []\n",
    "    acc_train = []\n",
    "    kl_train = []\n",
    "    mse_train = []\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    scheduler.step()\n",
    "    for batch_idx, (data, relations) in enumerate(train_loader):\n",
    "\n",
    "        if args.cuda:\n",
    "            data, relations = data.cuda(), relations.cuda()\n",
    "        data, relations = Variable(data), Variable(relations)\n",
    "        #print(\"train data shape\", data.shape) # \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ######## encoder\n",
    "        logits = encoder(data, rel_rec, rel_send)\n",
    "        #print(\"logits shape = \", logits.size())\n",
    "        edges = gumbel_softmax(logits, tau=args.temp, hard=args.hard)\n",
    "        prob = my_softmax(logits, -1)\n",
    "        ######## decoder\n",
    "        output = decoder(data, edges, rel_rec, rel_send, 100,\n",
    "                             burn_in=True,\n",
    "                             burn_in_steps=args.timesteps - args.prediction_steps)\n",
    "\n",
    "        target = data[:, :, 1:, :]\n",
    "        \n",
    "        ####### cal loss\n",
    "        loss_nll = nll_gaussian(output, target, args.var)\n",
    "\n",
    "        if args.prior:\n",
    "            loss_kl = kl_categorical(prob, log_prior, args.num_atoms)\n",
    "        else:\n",
    "            loss_kl = kl_categorical_uniform(prob, args.num_atoms,\n",
    "                                             args.edge_types)\n",
    "\n",
    "        loss = loss_nll + loss_kl\n",
    "        \n",
    "        \n",
    "        ##### other metric\n",
    "        acc = edge_accuracy(logits, relations)\n",
    "        acc_train.append(acc)\n",
    "        \n",
    "        ##### backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#        print('F.mse_loss(output, target).data', F.mse_loss(output, target).data)\n",
    "        mse_train.append(F.mse_loss(output, target).item())\n",
    "        nll_train.append(loss_nll.item())\n",
    "        kl_train.append(loss_kl.item())\n",
    "        \n",
    "        #break # test\n",
    "        \n",
    "    nll_val = []\n",
    "    acc_val = []\n",
    "    kl_val = []\n",
    "    mse_val = []\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    for batch_idx, (data, relations) in enumerate(valid_loader):\n",
    "        if args.cuda:\n",
    "            data, relations = data.cuda(), relations.cuda()\n",
    "        data, relations = Variable(data, volatile=True), Variable(\n",
    "            relations, volatile=True)\n",
    "\n",
    "        logits = encoder(data, rel_rec, rel_send)\n",
    "        edges = gumbel_softmax(logits, tau=args.temp, hard=True)\n",
    "        prob = my_softmax(logits, -1)\n",
    "\n",
    "        # validation output uses teacher forcing\n",
    "        output = decoder(data, edges, rel_rec, rel_send, 1)\n",
    "\n",
    "        target = data[:, :, 1:, :]\n",
    "        loss_nll = nll_gaussian(output, target, args.var)\n",
    "        loss_kl = kl_categorical_uniform(prob, args.num_atoms, args.edge_types)\n",
    "\n",
    "        acc = edge_accuracy(logits, relations)\n",
    "        acc_val.append(acc)\n",
    "\n",
    "        mse_val.append(F.mse_loss(output, target).item())\n",
    "        nll_val.append(loss_nll.item())\n",
    "        kl_val.append(loss_kl.item())\n",
    "\n",
    "    print('Epoch: {:04d}'.format(epoch),\n",
    "          'nll_train: {:.10f}'.format(np.mean(nll_train)),\n",
    "          'kl_train: {:.10f}'.format(np.mean(kl_train)),\n",
    "          'mse_train: {:.10f}'.format(np.mean(mse_train)),\n",
    "          'acc_train: {:.10f}'.format(np.mean(acc_train)),\n",
    "          'nll_val: {:.10f}'.format(np.mean(nll_val)),\n",
    "          'kl_val: {:.10f}'.format(np.mean(kl_val)),\n",
    "          'mse_val: {:.10f}'.format(np.mean(mse_val)),\n",
    "          'acc_val: {:.10f}'.format(np.mean(acc_val)),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "    if args.save_folder and np.mean(nll_val) < best_val_loss:\n",
    "        torch.save(encoder.state_dict(), encoder_file)\n",
    "        torch.save(decoder.state_dict(), decoder_file)\n",
    "        print('Best model so far, saving...')\n",
    "        print('Epoch: {:04d}'.format(epoch),\n",
    "              'nll_train: {:.10f}'.format(np.mean(nll_train)),\n",
    "              'kl_train: {:.10f}'.format(np.mean(kl_train)),\n",
    "              'mse_train: {:.10f}'.format(np.mean(mse_train)),\n",
    "              'acc_train: {:.10f}'.format(np.mean(acc_train)),\n",
    "              'nll_val: {:.10f}'.format(np.mean(nll_val)),\n",
    "              'kl_val: {:.10f}'.format(np.mean(kl_val)),\n",
    "              'mse_val: {:.10f}'.format(np.mean(mse_val)),\n",
    "              'acc_val: {:.10f}'.format(np.mean(acc_val)),\n",
    "              'time: {:.4f}s'.format(time.time() - t), file=log)\n",
    "        log.flush()\n",
    "    return np.mean(nll_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0000 nll_train: 4466.3222045898 kl_train: -0.4010268450 mse_train: 0.0023262086 acc_train: 0.5003333782 nll_val: 496.4222106934 kl_val: -0.5726509690 mse_val: 0.0002585532 acc_val: 0.4985000000 time: 15.9449s\n",
      "Best model so far, saving...\n",
      "Epoch: 0001 nll_train: 3095.7191772461 kl_train: -0.4224488437 mse_train: 0.0016123537 acc_train: 0.4950599407 nll_val: 227.0564422607 kl_val: -0.4177066684 mse_val: 0.0001182586 acc_val: 0.5160000000 time: 15.2731s\n",
      "Best model so far, saving...\n",
      "Epoch: 0002 nll_train: 1327.5251922607 kl_train: -0.3492365852 mse_train: 0.0006914194 acc_train: 0.4956694504 nll_val: 251.7565002441 kl_val: -0.2528773248 mse_val: 0.0001311232 acc_val: 0.5070000000 time: 15.5565s\n",
      "Epoch: 0003 nll_train: 1473.3057250977 kl_train: -0.2955962718 mse_train: 0.0007673464 acc_train: 0.4954808728 nll_val: 125.5177993774 kl_val: -0.2454427332 mse_val: 0.0000653739 acc_val: 0.5015000000 time: 15.3252s\n",
      "Best model so far, saving...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-be9c12116b03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_val_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_val_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mbest_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-72b062c82733>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, best_val_loss)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# validation output uses teacher forcing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_rec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_send\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-0ff2a0fa2a03>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, rel_type, rel_rec, rel_send, pred_steps, burn_in, burn_in_steps, dynamic_graph, encoder, temp)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0mrel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgumbel_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhard\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0;31m# sampled.. rel_type ([128, 20, 2])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             pred, hidden = self.single_step_forward(ins, rel_rec, rel_send,\n\u001b[0m\u001b[1;32m    188\u001b[0m                                                     rel_type, hidden)\n\u001b[1;32m    189\u001b[0m             \u001b[0mpred_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-0ff2a0fa2a03>\u001b[0m in \u001b[0;36msingle_step_forward\u001b[0;34m(self, inputs, rel_rec, rel_send, rel_type, hidden)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# single input (128, 5 , 4) -> (128, 5, 256), sum... GRU style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_r\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_r\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_h\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magg_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m   1624\u001b[0m     \"\"\"\n\u001b[1;32m   1625\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1626\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###### Train model\n",
    "t_total = time.time()\n",
    "best_val_loss = np.inf\n",
    "best_epoch = 0\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    val_loss = train(epoch, best_val_loss)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "    #break # test\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Best Epoch: {:04d}\".format(best_epoch))\n",
    "if args.save_folder:\n",
    "    print(\"Best Epoch: {:04d}\".format(best_epoch), file=log)\n",
    "    log.flush()\n"
   ]
  },
  {
   "source": [
    "# Test Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}